{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 09:32:15.014844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 09:32:15.038029: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-26 09:32:15.159129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-26 09:32:15.159207: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-26 09:32:15.178810: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-26 09:32:15.220611: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-26 09:32:15.221424: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 09:32:16.026282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/paddy/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAs\n",
    "- give the entire stock set and trade whether entire market is up or down\n",
    "- give entire stock set and pick stock that increases the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                gmtTime  askMedian  bidMedian  askVolume  bidVolume  \\\n",
      "250 2015-04-24 14:00:00      74.30      74.27     777.43     451.88   \n",
      "265 2015-04-24 15:00:00      74.45      74.43     520.67     480.03   \n",
      "271 2015-04-27 07:00:00      74.61      74.54     405.98     245.20   \n",
      "280 2015-04-27 08:00:00      74.57      74.54     238.85     190.07   \n",
      "292 2015-04-27 09:00:00      74.14      74.12     193.73     288.88   \n",
      "\n",
      "     spreadMedian  symbol  hour  day_of_week  askMedian_rolling_mean_3h  ...  \\\n",
      "250          0.03  STOCK1    14            4                  74.040000  ...   \n",
      "265          0.02  STOCK1    15            4                  74.223333  ...   \n",
      "271          0.06  STOCK1     7            0                  74.453333  ...   \n",
      "280          0.03  STOCK1     8            0                  74.543333  ...   \n",
      "292          0.03  STOCK1     9            0                  74.440000  ...   \n",
      "\n",
      "     askMedian_lag_22  bidMedian_lag_22  spreadMedian_lag_22  \\\n",
      "250             74.51             74.48                 0.03   \n",
      "265             74.52             74.49                 0.03   \n",
      "271             74.44             74.41                 0.03   \n",
      "280             74.60             74.58                 0.03   \n",
      "292             74.37             74.34                 0.03   \n",
      "\n",
      "     askMedian_lag_23  bidMedian_lag_23  spreadMedian_lag_23  \\\n",
      "250             74.81             74.78                 0.03   \n",
      "265             74.51             74.48                 0.03   \n",
      "271             74.52             74.49                 0.03   \n",
      "280             74.44             74.41                 0.03   \n",
      "292             74.60             74.58                 0.03   \n",
      "\n",
      "     askMedian_lag_24  bidMedian_lag_24  spreadMedian_lag_24  target  \n",
      "250             75.06             75.03                 0.03       1  \n",
      "265             74.81             74.78                 0.03       1  \n",
      "271             74.51             74.48                 0.03       1  \n",
      "280             74.52             74.49                 0.03       1  \n",
      "292             74.44             74.41                 0.03       1  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"/home/paddy/Documents/linc_hackathon/linc_hackathon_2025/given_resources/stockPrices_hourly.csv\")\n",
    "df[\"gmtTime\"] = pd.to_datetime(df[\"gmtTime\"])\n",
    "\n",
    "# Dictionary to store processed data for each stock\n",
    "stock_dfs = {}\n",
    "\n",
    "# Feature engineering for each stock\n",
    "for symbol in df[\"symbol\"].unique():\n",
    "    df_stock = df[df[\"symbol\"] == symbol].copy()\n",
    "\n",
    "    # Round numerical columns\n",
    "    cols_to_round = [col for col in df_stock.columns if col not in [\"gmtTime\", \"symbol\"]]\n",
    "    df_stock[cols_to_round] = df_stock[cols_to_round].round(2)\n",
    "\n",
    "    # Time-based features\n",
    "    df_stock['hour'] = df_stock['gmtTime'].dt.hour\n",
    "    df_stock['day_of_week'] = df_stock['gmtTime'].dt.dayofweek\n",
    "\n",
    "    # Rolling statistics\n",
    "    df_stock['askMedian_rolling_mean_3h'] = df_stock['askMedian'].rolling(window=3, min_periods=1).mean()\n",
    "    df_stock['bidMedian_rolling_mean_3h'] = df_stock['bidMedian'].rolling(window=3, min_periods=1).mean()\n",
    "    df_stock['askMedian_rolling_std_3h'] = df_stock['askMedian'].rolling(window=3, min_periods=1).std()\n",
    "    df_stock['bidMedian_rolling_std_3h'] = df_stock['bidMedian'].rolling(window=3, min_periods=1).std()\n",
    "\n",
    "    # Percentage changes\n",
    "    df_stock['askMedian_pct_change'] = df_stock['askMedian'].pct_change()\n",
    "    df_stock['bidMedian_pct_change'] = df_stock['bidMedian'].pct_change()\n",
    "\n",
    "    # Spread-related features\n",
    "    df_stock['spread_ratio'] = df_stock['spreadMedian'] / (df_stock['askMedian'] + df_stock['bidMedian'])\n",
    "    # df_stock['spread_pct_change'] = df_stock['spreadMedian'].pct_change()\n",
    "\n",
    "    # Volume-related features\n",
    "    df_stock['askVolume_relative'] = df_stock['askVolume'] / df_stock['askVolume'].rolling(window=5, min_periods=1).mean()\n",
    "    df_stock['bidVolume_relative'] = df_stock['bidVolume'] / df_stock['bidVolume'].rolling(window=5, min_periods=1).mean()\n",
    "    df_stock['volume_imbalance'] = (df_stock['askVolume'] - df_stock['bidVolume']) / (df_stock['askVolume'] + df_stock['bidVolume'])\n",
    "\n",
    "    # Lagged features (e.g., previous hour's values)\n",
    "    for lag in range(1, 25):  # Add lags for the last 3 hours\n",
    "        df_stock[f'askMedian_lag_{lag}'] = df_stock['askMedian'].shift(lag)\n",
    "        df_stock[f'bidMedian_lag_{lag}'] = df_stock['bidMedian'].shift(lag)\n",
    "        df_stock[f'spreadMedian_lag_{lag}'] = df_stock['spreadMedian'].shift(lag)\n",
    "\n",
    "    # Target variable: Direction of price movement (1 if bidMedian increases next hour, 0 otherwise)\n",
    "    df_stock['target'] = (df_stock['bidMedian'].shift(-5) > df_stock['bidMedian']).astype(int)\n",
    "\n",
    "    # Drop rows with missing values (due to lags and rolling features)\n",
    "    df_stock = df_stock.dropna()\n",
    "\n",
    "    # Store processed dataframe\n",
    "    stock_dfs[symbol] = df_stock\n",
    "\n",
    "# Example: View processed data for one stock\n",
    "print(stock_dfs['STOCK1'].head())\n",
    "\n",
    "# Save processed data to CSV (optional)\n",
    "for symbol, df_stock in stock_dfs.items():\n",
    "    df_stock.to_csv(f\"{symbol}_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "369/369 [==============================] - 5s 9ms/step - loss: 0.6947 - accuracy: 0.5114 - val_loss: 0.6923 - val_accuracy: 0.5233\n",
      "Epoch 2/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6924 - accuracy: 0.5163 - val_loss: 0.6939 - val_accuracy: 0.5223\n",
      "Epoch 3/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6899 - accuracy: 0.5314 - val_loss: 0.6956 - val_accuracy: 0.5178\n",
      "Epoch 4/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6856 - accuracy: 0.5424 - val_loss: 0.7210 - val_accuracy: 0.5052\n",
      "Epoch 5/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6836 - accuracy: 0.5451 - val_loss: 0.6960 - val_accuracy: 0.5292\n",
      "Epoch 6/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6751 - accuracy: 0.5632 - val_loss: 0.7430 - val_accuracy: 0.5110\n",
      "Epoch 7/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6688 - accuracy: 0.5686 - val_loss: 0.7297 - val_accuracy: 0.5187\n",
      "Epoch 8/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6603 - accuracy: 0.5863 - val_loss: 0.7469 - val_accuracy: 0.5197\n",
      "Epoch 9/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6491 - accuracy: 0.5955 - val_loss: 0.7805 - val_accuracy: 0.5267\n",
      "Epoch 10/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6365 - accuracy: 0.6072 - val_loss: 0.8140 - val_accuracy: 0.5315\n",
      "Epoch 11/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6227 - accuracy: 0.6271 - val_loss: 0.8057 - val_accuracy: 0.5201\n",
      "Epoch 12/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.6068 - accuracy: 0.6317 - val_loss: 0.8709 - val_accuracy: 0.5243\n",
      "Epoch 13/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5907 - accuracy: 0.6589 - val_loss: 0.8812 - val_accuracy: 0.5204\n",
      "Epoch 14/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5755 - accuracy: 0.6680 - val_loss: 0.9300 - val_accuracy: 0.5209\n",
      "Epoch 15/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5590 - accuracy: 0.6856 - val_loss: 0.9913 - val_accuracy: 0.5250\n",
      "Epoch 16/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5485 - accuracy: 0.6913 - val_loss: 1.0227 - val_accuracy: 0.5303\n",
      "Epoch 17/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5335 - accuracy: 0.7003 - val_loss: 1.0573 - val_accuracy: 0.5281\n",
      "Epoch 18/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5131 - accuracy: 0.7190 - val_loss: 1.1087 - val_accuracy: 0.5289\n",
      "Epoch 19/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.5064 - accuracy: 0.7267 - val_loss: 1.0934 - val_accuracy: 0.5223\n",
      "Epoch 20/20\n",
      "369/369 [==============================] - 3s 8ms/step - loss: 0.4900 - accuracy: 0.7367 - val_loss: 1.1365 - val_accuracy: 0.5295\n",
      "371/371 [==============================] - 1s 2ms/step - loss: 1.1365 - accuracy: 0.5295\n",
      "Test Accuracy: 0.5295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paddy/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Load processed data (Example for one stock, modify as needed)\n",
    "df = pd.read_csv(\"STOCK2_processed.csv\")\n",
    "\n",
    "# Select features and target\n",
    "features = [col for col in df.columns if col not in ['gmtTime', 'symbol', 'target']]\n",
    "target = 'target'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[features])\n",
    "y = df[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_train = X_seq\n",
    "y_train = y_seq\n",
    "\n",
    "df_test = pd.read_csv(\"STOCK3_processed.csv\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_test[features])\n",
    "y = df_test[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_test = X_seq\n",
    "y_test = y_seq\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, X_train.shape[2]))\n",
    ")\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100, return_sequences=True))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(25, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_trading_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entire dataset trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1843/1843 [==============================] - 17s 8ms/step - loss: 0.6936 - accuracy: 0.5074 - val_loss: 0.6940 - val_accuracy: 0.5064\n",
      "Epoch 2/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6928 - accuracy: 0.5065 - val_loss: 0.6929 - val_accuracy: 0.5080\n",
      "Epoch 3/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5097\n",
      "Epoch 4/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6922 - accuracy: 0.5054 - val_loss: 0.6944 - val_accuracy: 0.5029\n",
      "Epoch 5/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6916 - accuracy: 0.5140 - val_loss: 0.6919 - val_accuracy: 0.5102\n",
      "Epoch 6/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6905 - accuracy: 0.5151 - val_loss: 0.6915 - val_accuracy: 0.5100\n",
      "Epoch 7/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6890 - accuracy: 0.5173 - val_loss: 0.6911 - val_accuracy: 0.5109\n",
      "Epoch 8/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6874 - accuracy: 0.5237 - val_loss: 0.6911 - val_accuracy: 0.5180\n",
      "Epoch 9/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6853 - accuracy: 0.5280 - val_loss: 0.6909 - val_accuracy: 0.5184\n",
      "Epoch 10/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6830 - accuracy: 0.5296 - val_loss: 0.6903 - val_accuracy: 0.5196\n",
      "Epoch 11/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6800 - accuracy: 0.5373 - val_loss: 0.6887 - val_accuracy: 0.5249\n",
      "Epoch 12/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6769 - accuracy: 0.5402 - val_loss: 0.6954 - val_accuracy: 0.5253\n",
      "Epoch 13/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6737 - accuracy: 0.5440 - val_loss: 0.6966 - val_accuracy: 0.5293\n",
      "Epoch 14/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6694 - accuracy: 0.5517 - val_loss: 0.6869 - val_accuracy: 0.5302\n",
      "Epoch 15/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6650 - accuracy: 0.5553 - val_loss: 0.6938 - val_accuracy: 0.5345\n",
      "Epoch 16/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6612 - accuracy: 0.5623 - val_loss: 0.6947 - val_accuracy: 0.5375\n",
      "Epoch 17/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6578 - accuracy: 0.5636 - val_loss: 0.6901 - val_accuracy: 0.5382\n",
      "Epoch 18/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6540 - accuracy: 0.5659 - val_loss: 0.6913 - val_accuracy: 0.5435\n",
      "Epoch 19/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6488 - accuracy: 0.5735 - val_loss: 0.6927 - val_accuracy: 0.5449\n",
      "Epoch 20/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6451 - accuracy: 0.5762 - val_loss: 0.6909 - val_accuracy: 0.5461\n",
      "1843/1843 [==============================] - 4s 2ms/step - loss: 0.6909 - accuracy: 0.5461\n",
      "Test Accuracy: 0.5461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paddy/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "stock_files = [\"STOCK1_processed.csv\", \"STOCK2_processed.csv\", \"STOCK3_processed.csv\", \"STOCK4_processed.csv\", \"STOCK5_processed.csv\", \"STOCK6_processed.csv\", \"STOCK7_processed.csv\",\"STOCK8_processed.csv\",\"STOCK9_processed.csv\",\"STOCK10_processed.csv\"]  # Add more stock files as needed\n",
    "\n",
    "# Load and combine all stock data\n",
    "dfs = []\n",
    "for file in stock_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, ignore_index=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dfs[features])\n",
    "y = dfs[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_test = X_seq\n",
    "y_test = y_seq\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, X_train.shape[2]))\n",
    ")\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100, return_sequences=True))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(25, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_trading_model_entire_set.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
