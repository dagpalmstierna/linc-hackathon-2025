{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDEAs\n",
    "- give the entire stock set and trade whether entire market is up or down\n",
    "- give entire stock set and pick stock that increases the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                gmtTime  askMedian  bidMedian  askVolume  bidVolume  \\\n",
      "250 2015-04-24 14:00:00      74.30      74.27     777.43     451.88   \n",
      "265 2015-04-24 15:00:00      74.45      74.43     520.67     480.03   \n",
      "271 2015-04-27 07:00:00      74.61      74.54     405.98     245.20   \n",
      "280 2015-04-27 08:00:00      74.57      74.54     238.85     190.07   \n",
      "292 2015-04-27 09:00:00      74.14      74.12     193.73     288.88   \n",
      "\n",
      "     spreadMedian  symbol  year  month  hour  ...  askMedian_lag_22  \\\n",
      "250          0.03  STOCK1  2015      4    14  ...             74.51   \n",
      "265          0.02  STOCK1  2015      4    15  ...             74.52   \n",
      "271          0.06  STOCK1  2015      4     7  ...             74.44   \n",
      "280          0.03  STOCK1  2015      4     8  ...             74.60   \n",
      "292          0.03  STOCK1  2015      4     9  ...             74.37   \n",
      "\n",
      "     bidMedian_lag_22  spreadMedian_lag_22  askMedian_lag_23  \\\n",
      "250             74.48                 0.03             74.81   \n",
      "265             74.49                 0.03             74.51   \n",
      "271             74.41                 0.03             74.52   \n",
      "280             74.58                 0.03             74.44   \n",
      "292             74.34                 0.03             74.60   \n",
      "\n",
      "     bidMedian_lag_23  spreadMedian_lag_23  askMedian_lag_24  \\\n",
      "250             74.78                 0.03             75.06   \n",
      "265             74.48                 0.03             74.81   \n",
      "271             74.49                 0.03             74.51   \n",
      "280             74.41                 0.03             74.52   \n",
      "292             74.58                 0.03             74.44   \n",
      "\n",
      "     bidMedian_lag_24  spreadMedian_lag_24  target  \n",
      "250             75.03                 0.03       1  \n",
      "265             74.78                 0.03       1  \n",
      "271             74.48                 0.03       1  \n",
      "280             74.49                 0.03       1  \n",
      "292             74.41                 0.03       1  \n",
      "\n",
      "[5 rows x 94 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"/home/paddy/Documents/linc_hackathon/linc_hackathon_2025/given_resources/stockPrices_hourly.csv\")\n",
    "df[\"gmtTime\"] = pd.to_datetime(df[\"gmtTime\"])\n",
    "\n",
    "# Dictionary to store processed data for each stock\n",
    "stock_dfs = {}\n",
    "\n",
    "# Feature engineering for each stock\n",
    "for symbol in df[\"symbol\"].unique():\n",
    "    df_stock = df[df[\"symbol\"] == symbol].copy()\n",
    "\n",
    "    # Round numerical columns\n",
    "    cols_to_round = [col for col in df_stock.columns if col not in [\"gmtTime\", \"symbol\"]]\n",
    "    df_stock[cols_to_round] = df_stock[cols_to_round].round(2)\n",
    "\n",
    "    # Time-based features\n",
    "    df_stock['hour'] = df_stock['gmtTime'].dt.hour\n",
    "    df_stock['day_of_week'] = df_stock['gmtTime'].dt.dayofweek\n",
    "\n",
    "    # Rolling statistics\n",
    "    df_stock['askMedian_rolling_mean_3h'] = df_stock['askMedian'].rolling(window=3, min_periods=1).mean()\n",
    "    df_stock['bidMedian_rolling_mean_3h'] = df_stock['bidMedian'].rolling(window=3, min_periods=1).mean()\n",
    "    df_stock['askMedian_rolling_std_3h'] = df_stock['askMedian'].rolling(window=3, min_periods=1).std()\n",
    "    df_stock['bidMedian_rolling_std_3h'] = df_stock['bidMedian'].rolling(window=3, min_periods=1).std()\n",
    "\n",
    "    # Percentage changes\n",
    "    df_stock['askMedian_pct_change'] = df_stock['askMedian'].pct_change()\n",
    "    df_stock['bidMedian_pct_change'] = df_stock['bidMedian'].pct_change()\n",
    "\n",
    "    # Spread-related features\n",
    "    df_stock['spread_ratio'] = df_stock['spreadMedian'] / (df_stock['askMedian'] + df_stock['bidMedian'])\n",
    "    # df_stock['spread_pct_change'] = df_stock['spreadMedian'].pct_change()\n",
    "\n",
    "    # Volume-related features\n",
    "    df_stock['askVolume_relative'] = df_stock['askVolume'] / df_stock['askVolume'].rolling(window=5, min_periods=1).mean()\n",
    "    df_stock['bidVolume_relative'] = df_stock['bidVolume'] / df_stock['bidVolume'].rolling(window=5, min_periods=1).mean()\n",
    "    df_stock['volume_imbalance'] = (df_stock['askVolume'] - df_stock['bidVolume']) / (df_stock['askVolume'] + df_stock['bidVolume'])\n",
    "\n",
    "    # Lagged features (e.g., previous hour's values)\n",
    "    for lag in range(1, 25):  # Add lags for the last 3 hours\n",
    "        df_stock[f'askMedian_lag_{lag}'] = df_stock['askMedian'].shift(lag)\n",
    "        df_stock[f'bidMedian_lag_{lag}'] = df_stock['bidMedian'].shift(lag)\n",
    "        df_stock[f'spreadMedian_lag_{lag}'] = df_stock['spreadMedian'].shift(lag)\n",
    "\n",
    "    # Target variable: Direction of price movement (1 if bidMedian increases next hour, 0 otherwise)\n",
    "    df_stock['target'] = (df_stock['bidMedian'].shift(-5) > df_stock['bidMedian']).astype(int)\n",
    "\n",
    "    # Drop rows with missing values (due to lags and rolling features)\n",
    "    df_stock = df_stock.dropna()\n",
    "\n",
    "    # Store processed dataframe\n",
    "    stock_dfs[symbol] = df_stock\n",
    "\n",
    "# Example: View processed data for one stock\n",
    "print(stock_dfs['STOCK1'].head())\n",
    "\n",
    "# Save processed data to CSV (optional)\n",
    "for symbol, df_stock in stock_dfs.items():\n",
    "    df_stock.to_csv(f\"{symbol}_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "369/369 [==============================] - 11s 21ms/step - loss: 0.6961 - accuracy: 0.5053 - val_loss: 0.6935 - val_accuracy: 0.4879\n",
      "Epoch 2/20\n",
      "369/369 [==============================] - 7s 19ms/step - loss: 0.6938 - accuracy: 0.5110 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 3/20\n",
      "369/369 [==============================] - 7s 19ms/step - loss: 0.6933 - accuracy: 0.5102 - val_loss: 0.6927 - val_accuracy: 0.5121\n",
      "Epoch 4/20\n",
      "369/369 [==============================] - 7s 19ms/step - loss: 0.6932 - accuracy: 0.5064 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 5/20\n",
      "369/369 [==============================] - 7s 18ms/step - loss: 0.6932 - accuracy: 0.5060 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 6/20\n",
      "369/369 [==============================] - 6s 18ms/step - loss: 0.6931 - accuracy: 0.5127 - val_loss: 0.6932 - val_accuracy: 0.5121\n",
      "Epoch 7/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6931 - accuracy: 0.5086 - val_loss: 0.6932 - val_accuracy: 0.4879\n",
      "Epoch 8/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6935 - accuracy: 0.5062 - val_loss: 0.6928 - val_accuracy: 0.5121\n",
      "Epoch 9/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6931 - accuracy: 0.5073 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 10/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6933 - accuracy: 0.5103 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 11/20\n",
      "369/369 [==============================] - 6s 17ms/step - loss: 0.6932 - accuracy: 0.5063 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 12/20\n",
      "369/369 [==============================] - 7s 18ms/step - loss: 0.6932 - accuracy: 0.5075 - val_loss: 0.6931 - val_accuracy: 0.5121\n",
      "Epoch 13/20\n",
      "369/369 [==============================] - 7s 18ms/step - loss: 0.6930 - accuracy: 0.5098 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 14/20\n",
      "369/369 [==============================] - 7s 18ms/step - loss: 0.6930 - accuracy: 0.5097 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 15/20\n",
      "369/369 [==============================] - 8s 21ms/step - loss: 0.6931 - accuracy: 0.5066 - val_loss: 0.6930 - val_accuracy: 0.5121\n",
      "Epoch 16/20\n",
      "369/369 [==============================] - 7s 18ms/step - loss: 0.6930 - accuracy: 0.5088 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 17/20\n",
      "369/369 [==============================] - 6s 17ms/step - loss: 0.6930 - accuracy: 0.5058 - val_loss: 0.6934 - val_accuracy: 0.4879\n",
      "Epoch 18/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6933 - accuracy: 0.5054 - val_loss: 0.6932 - val_accuracy: 0.5121\n",
      "Epoch 19/20\n",
      "369/369 [==============================] - 6s 16ms/step - loss: 0.6931 - accuracy: 0.5102 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "Epoch 20/20\n",
      "369/369 [==============================] - 6s 17ms/step - loss: 0.6932 - accuracy: 0.5058 - val_loss: 0.6929 - val_accuracy: 0.5121\n",
      "371/371 [==============================] - 2s 4ms/step - loss: 0.6929 - accuracy: 0.5121\n",
      "Test Accuracy: 0.5121\n"
     ]
    }
   ],
   "source": [
    "# Load processed data (Example for one stock, modify as needed)\n",
    "df = pd.read_csv(\"STOCK2_processed.csv\")\n",
    "\n",
    "# Select features and target\n",
    "features = [col for col in df.columns if col not in ['gmtTime', 'symbol', 'target']]\n",
    "target = 'target'\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df[features])\n",
    "y = df[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_train = X_seq\n",
    "y_train = y_seq\n",
    "\n",
    "df_test = pd.read_csv(\"STOCK3_processed.csv\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_test[features])\n",
    "y = df_test[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_test = X_seq\n",
    "y_test = y_seq\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, X_train.shape[2]))\n",
    ")\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100, return_sequences=True))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(25, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_alt = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(sequence_length,  X_train.shape[2])),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    GRU(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    GRU(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model = model_2\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_trading_model_previous_year.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entire dataset trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1843/1843 [==============================] - 17s 8ms/step - loss: 0.6936 - accuracy: 0.5074 - val_loss: 0.6940 - val_accuracy: 0.5064\n",
      "Epoch 2/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6928 - accuracy: 0.5065 - val_loss: 0.6929 - val_accuracy: 0.5080\n",
      "Epoch 3/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6924 - accuracy: 0.5096 - val_loss: 0.6924 - val_accuracy: 0.5097\n",
      "Epoch 4/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6922 - accuracy: 0.5054 - val_loss: 0.6944 - val_accuracy: 0.5029\n",
      "Epoch 5/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6916 - accuracy: 0.5140 - val_loss: 0.6919 - val_accuracy: 0.5102\n",
      "Epoch 6/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6905 - accuracy: 0.5151 - val_loss: 0.6915 - val_accuracy: 0.5100\n",
      "Epoch 7/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6890 - accuracy: 0.5173 - val_loss: 0.6911 - val_accuracy: 0.5109\n",
      "Epoch 8/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6874 - accuracy: 0.5237 - val_loss: 0.6911 - val_accuracy: 0.5180\n",
      "Epoch 9/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6853 - accuracy: 0.5280 - val_loss: 0.6909 - val_accuracy: 0.5184\n",
      "Epoch 10/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6830 - accuracy: 0.5296 - val_loss: 0.6903 - val_accuracy: 0.5196\n",
      "Epoch 11/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6800 - accuracy: 0.5373 - val_loss: 0.6887 - val_accuracy: 0.5249\n",
      "Epoch 12/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6769 - accuracy: 0.5402 - val_loss: 0.6954 - val_accuracy: 0.5253\n",
      "Epoch 13/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6737 - accuracy: 0.5440 - val_loss: 0.6966 - val_accuracy: 0.5293\n",
      "Epoch 14/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6694 - accuracy: 0.5517 - val_loss: 0.6869 - val_accuracy: 0.5302\n",
      "Epoch 15/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6650 - accuracy: 0.5553 - val_loss: 0.6938 - val_accuracy: 0.5345\n",
      "Epoch 16/20\n",
      "1843/1843 [==============================] - 14s 8ms/step - loss: 0.6612 - accuracy: 0.5623 - val_loss: 0.6947 - val_accuracy: 0.5375\n",
      "Epoch 17/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6578 - accuracy: 0.5636 - val_loss: 0.6901 - val_accuracy: 0.5382\n",
      "Epoch 18/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6540 - accuracy: 0.5659 - val_loss: 0.6913 - val_accuracy: 0.5435\n",
      "Epoch 19/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6488 - accuracy: 0.5735 - val_loss: 0.6927 - val_accuracy: 0.5449\n",
      "Epoch 20/20\n",
      "1843/1843 [==============================] - 14s 7ms/step - loss: 0.6451 - accuracy: 0.5762 - val_loss: 0.6909 - val_accuracy: 0.5461\n",
      "1843/1843 [==============================] - 4s 2ms/step - loss: 0.6909 - accuracy: 0.5461\n",
      "Test Accuracy: 0.5461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paddy/anaconda3/lib/python3.9/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "stock_files = [\"STOCK1_processed.csv\", \"STOCK2_processed.csv\", \"STOCK3_processed.csv\", \"STOCK4_processed.csv\", \"STOCK5_processed.csv\", \"STOCK6_processed.csv\", \"STOCK7_processed.csv\",\"STOCK8_processed.csv\",\"STOCK9_processed.csv\",\"STOCK10_processed.csv\"]  # Add more stock files as needed\n",
    "\n",
    "# Load and combine all stock data\n",
    "dfs = []\n",
    "for file in stock_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, ignore_index=True)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dfs[features])\n",
    "y = dfs[target].values\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features)\n",
    "sequence_length = 10  # Use last 10 hours as input\n",
    "X_seq = []\n",
    "y_seq = []\n",
    "\n",
    "for i in range(len(X) - sequence_length):\n",
    "    X_seq.append(X[i:i+sequence_length])\n",
    "    y_seq.append(y[i+sequence_length])\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "X_test = X_seq\n",
    "y_test = y_seq\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(sequence_length, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(\n",
    "    LSTM(100, return_sequences=True, input_shape=(sequence_length, X_train.shape[2]))\n",
    ")\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100, return_sequences=True))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(LSTM(100))\n",
    "model_2.add(Dropout(0.2))\n",
    "model_2.add(Dense(25, activation='relu'))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_trading_model_entire_set.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
